import json
import os
import asyncio
import random
import re
from playwright.async_api import async_playwright
from datetime import datetime

# --- è¨­å®š ---
BATCH_SIZE = 150  # 1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹ä»¶æ•° (å¢—é‡ï¼)
DATA_FILE = 'collect.json'
AUTH_FILE = 'auth.json'

# æ•°å€¤å¤‰æ› (1.5ä¸‡ -> 15000)
def parse_metric(text):
    if not text: return 0
    text = text.replace(',', '').strip()
    try:
        if 'ä¸‡' in text: return int(float(text.replace('ä¸‡', '')) * 10000)
        if 'K' in text: return int(float(text.replace('K', '')) * 1000)
        if 'M' in text: return int(float(text.replace('M', '')) * 1000000)
        # æ•°å­—ä»¥å¤–ã‚’é™¤å»ã—ã¦å¤‰æ›
        return int(''.join(filter(str.isdigit, text)) or 0)
    except: return 0

async def fetch_metrics():
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not os.path.exists(DATA_FILE): return
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé¸å®š: ã€Œã„ã„ã­ãŒç„¡ã„ã€ã¾ãŸã¯ã€Œæœ¬æ–‡(text)ãŒç„¡ã„ã€ãƒ‡ãƒ¼ã‚¿
    # ã‹ã¤ã€ã¾ã ã‚¨ãƒ©ãƒ¼ã§å¼¾ã‹ã‚Œã¦ã„ãªã„ã‚‚ã®ï¼ˆlast_fetchedãŒãªã„ã€ã¾ãŸã¯å¤ã„ï¼‰
    targets = [d for d in data if d.get('like_count', 0) == 0 or 'text' not in d]
    
    current_batch = targets[:BATCH_SIZE]
    
    print(f"ğŸ¯ ä»Šå›ã®å–å¾—å¯¾è±¡: {len(current_batch)} ä»¶ / æ®‹ã‚Š {len(targets)} ä»¶")

    if not current_batch:
        print("âœ… å…¨ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤ãƒ»æœ¬æ–‡å–å¾—ãŒå®Œäº†ã—ã¦ã„ã¾ã™ï¼")
        return

    # 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        # ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ãŒã‚ã‚Œã°ä½¿ã†
        context_options = {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        # auth.jsonã®ä¸­èº«ã‚’ç›´æ¥èª­ã¿è¾¼ã‚“ã§cookiesã¨ã—ã¦æ¸¡ã™æ–¹ãŒç¢ºå®Ÿãªå ´åˆãŒå¤šã„
        if os.path.exists(AUTH_FILE):
             with open(AUTH_FILE, 'r') as f:
                cookies = json.load(f)
                context = await browser.new_context(**context_options)
                await context.add_cookies(cookies)
        else:
            context = await browser.new_context(**context_options)

        page = await context.new_page()

        processed_count = 0
        for i, item in enumerate(current_batch):
            url = item['url']
            print(f"[{i+1}/{len(current_batch)}] Accessing: {url}")

            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—é•·ã‚ã«
                await page.goto(url, wait_until="domcontentloaded", timeout=45000)
                
                # ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤ï¼ˆæœ€å¤§10ç§’ï¼‰
                try:
                    await page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)
                except:
                    print("  âš ï¸ Tweet content not found (deleted or sensitive?)")

                await asyncio.sleep(random.uniform(1.5, 3.5)) # å¾…æ©Ÿ

                # --- A. æ•°å€¤å–å¾— ---
                likes = 0
                views = 0
                
                # ã„ã„ã­æ•°: aria-label="150 likes" ã‚’ç‹™ã†ã®ãŒä¸€ç•ªç¢ºå®Ÿ
                like_elem = await page.query_selector('[data-testid="like"]')
                if like_elem:
                    aria = await like_elem.get_attribute('aria-label')
                    if aria:
                        match = re.search(r'(\d[\d,.]*[KkMmä¸‡]?)', aria)
                        if match: likes = parse_metric(match.group(1))
                
                # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
                view_elem = await page.query_selector('a[href$="/analytics"]')
                if view_elem:
                    aria = await view_elem.get_attribute('aria-label')
                    if aria:
                        match = re.search(r'(\d[\d,.]*[KkMmä¸‡]?)', aria)
                        if match: views = parse_metric(match.group(1))

                # --- B. æœ¬æ–‡å–å¾— ---
                text_content = ""
                text_elem = await page.query_selector('[data-testid="tweetText"]')
                if text_elem:
                    text_content = await text_elem.inner_text()
                    text_content = text_content.replace('\n', ' ')

                # --- C. ãƒ‡ãƒ¼ã‚¿æ›´æ–° ---
                item['like_count'] = likes
                item['impression_count'] = views
                item['text'] = text_content
                item['last_fetched'] = datetime.now().isoformat()

                if text_content:
                    print(f"   âœ… Likes: {likes}, Text: {text_content[:20]}...")
                else:
                    print(f"   âœ… Likes: {likes} (No Text)")

                processed_count += 1
                
                # ã“ã¾ã‚ã«ä¿å­˜ (5ä»¶ã”ã¨)
                if processed_count % 5 == 0:
                    with open(DATA_FILE, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"   âŒ Error: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç©ºãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã¦æ›´æ–°æ™‚åˆ»ã‚’è¨˜éŒ²ã—ã€ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²ã
                item['like_count'] = 0
                item['text'] = ""
                item['last_fetched'] = datetime.now().isoformat()
                continue

        await browser.close()

    # æœ€çµ‚ä¿å­˜
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ¨ ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼ {processed_count} ä»¶æ›´æ–°ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    asyncio.run(fetch_metrics())