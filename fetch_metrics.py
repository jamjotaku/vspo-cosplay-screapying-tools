import json
import os
import asyncio
import random
import re
from playwright.async_api import async_playwright
from datetime import datetime

# --- è¨­å®š ---
BATCH_SIZE = 150  # 1å›ã®å®Ÿè¡Œã§å‡¦ç†ã™ã‚‹ä»¶æ•°
DATA_FILE = 'collect.json'
AUTH_FILE = 'auth.json'

# æ•°å€¤å¤‰æ› (1.5ä¸‡ -> 15000)
def parse_metric(text):
    if not text: return 0
    text = text.replace(',', '').strip()
    try:
        if 'ä¸‡' in text: return int(float(text.replace('ä¸‡', '')) * 10000)
        if 'K' in text: return int(float(text.replace('K', '')) * 1000)
        if 'M' in text: return int(float(text.replace('M', '')) * 1000000)
        return int(''.join(filter(str.isdigit, text)) or 0)
    except: return 0

async def fetch_metrics():
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if not os.path.exists(DATA_FILE): return
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé¸å®š
    targets = [d for d in data if d.get('like_count', 0) == 0 or 'text' not in d]
    current_batch = targets[:BATCH_SIZE]
    
    print(f"ğŸ¯ ä»Šå›ã®å–å¾—å¯¾è±¡: {len(current_batch)} ä»¶ / æ®‹ã‚Š {len(targets)} ä»¶")

    if not current_batch:
        print("âœ… å…¨ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤ãƒ»æœ¬æ–‡å–å¾—ãŒå®Œäº†ã—ã¦ã„ã¾ã™ï¼")
        return

    # 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®è¨­å®š
        context_options = {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        # â˜…ã“ã“ã‚’ä¿®æ­£ã—ã¾ã—ãŸâ˜…
        # auth.json ã‚’ç›´æ¥ storage_state ã¨ã—ã¦æ¸¡ã™ã“ã¨ã§ã€å½¢å¼ã®é•ã„ã‚’è‡ªå‹•å¸åã•ã›ã¾ã™
        if os.path.exists(AUTH_FILE):
            context_options["storage_state"] = AUTH_FILE

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        context = await browser.new_context(**context_options)
        page = await context.new_page()

        processed_count = 0
        for i, item in enumerate(current_batch):
            url = item['url']
            print(f"[{i+1}/{len(current_batch)}] Accessing: {url}")

            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’45ç§’ã«è¨­å®š
                await page.goto(url, wait_until="domcontentloaded", timeout=45000)
                
                try:
                    await page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)
                except:
                    print("  âš ï¸ Tweet content not found (deleted or sensitive?)")

                await asyncio.sleep(random.uniform(1.5, 3.5)) 

                # --- A. æ•°å€¤å–å¾— ---
                likes = 0
                views = 0
                
                # ã„ã„ã­æ•°
                like_elem = await page.query_selector('[data-testid="like"]')
                if like_elem:
                    aria = await like_elem.get_attribute('aria-label')
                    if aria:
                        match = re.search(r'(\d[\d,.]*[KkMmä¸‡]?)', aria)
                        if match: likes = parse_metric(match.group(1))
                
                # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
                view_elem = await page.query_selector('a[href$="/analytics"]')
                if view_elem:
                    aria = await view_elem.get_attribute('aria-label')
                    if aria:
                        match = re.search(r'(\d[\d,.]*[KkMmä¸‡]?)', aria)
                        if match: views = parse_metric(match.group(1))

                # --- B. æœ¬æ–‡å–å¾— ---
                text_content = ""
                text_elem = await page.query_selector('[data-testid="tweetText"]')
                if text_elem:
                    text_content = await text_elem.inner_text()
                    text_content = text_content.replace('\n', ' ')

                # --- C. ãƒ‡ãƒ¼ã‚¿æ›´æ–° ---
                item['like_count'] = likes
                item['impression_count'] = views
                item['text'] = text_content
                item['last_fetched'] = datetime.now().isoformat()

                if text_content:
                    print(f"   âœ… Likes: {likes}, Text: {text_content[:20]}...")
                else:
                    print(f"   âœ… Likes: {likes} (No Text)")

                processed_count += 1
                
                # 5ä»¶ã”ã¨ã«ä¿å­˜
                if processed_count % 5 == 0:
                    with open(DATA_FILE, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"   âŒ Error: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
                item['like_count'] = 0
                item['text'] = ""
                item['last_fetched'] = datetime.now().isoformat()
                continue

        await browser.close()

    # æœ€çµ‚ä¿å­˜
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ¨ ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼ {processed_count} ä»¶æ›´æ–°ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    asyncio.run(fetch_metrics())