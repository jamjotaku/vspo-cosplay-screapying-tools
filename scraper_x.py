import json
import os
import re
import asyncio
import random
from datetime import datetime
from playwright.async_api import async_playwright

def extract_number(text, pattern):
    if not text: return "0"
    match = re.search(pattern, text)
    return match.group(1) if match else "0"

async def scrape_vspo_cosplay(context, member):
    results = []
    page = await context.new_page()
    # ç”»é¢ã‚µã‚¤ã‚ºã‚’å›ºå®šã—ã¦ã€ãƒ¢ãƒã‚¤ãƒ«ç‰ˆã¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’é˜²ã
    await page.set_viewport_size({"width": 1280, "height": 800})

    query = f"{member['name']} ã‚³ã‚¹ãƒ—ãƒ¬"
    url = f"https://x.com/search?q={query}&src=typed_query&f=live"
    
    print(f"--- Searching for: {member['name']} ---")
    try:
        # 1. ãƒšãƒ¼ã‚¸ç§»å‹•ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè½ã¡ç€ãã¾ã§å¾…æ©Ÿï¼‰
        await page.goto(url, wait_until="networkidle", timeout=60000)
        
        # 2. äººé–“ã‚‰ã—ããƒ©ãƒ³ãƒ€ãƒ ã«å¾…æ©Ÿ
        wait_time = random.uniform(5000, 8000)
        await page.wait_for_timeout(wait_time)

        # 3. ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«é£›ã°ã•ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if "login" in page.url:
            print(f"âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ãŒè§£é™¤ã•ã‚Œã¦ã„ã¾ã™ã€‚Cookieã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
            await page.screenshot(path="login_error.png")
            return []

        # 4. ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§æœ€å¤§20ç§’å¾…æ©Ÿ
        try:
            await page.wait_for_selector('article[data-testid="tweet"]', timeout=20000)
        except:
            print(f"âŒ ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæ¤œç´¢çµæœ0ä»¶ã€ã¾ãŸã¯ãƒ–ãƒ­ãƒƒã‚¯ï¼‰")
            await page.screenshot(path=f"not_found_{member['id']}.png")
            return []

        # 5. ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦èª­ã¿è¾¼ã¿ã‚’ä¿ƒã™
        await page.mouse.wheel(0, 2000)
        await asyncio.sleep(2)

        tweets = await page.query_selector_all('article[data-testid="tweet"]')
        print(f"âœ… Found {len(tweets)} potential tweets")

        for tweet in tweets:
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
                user_info = await tweet.query_selector('[data-testid="User-Name"]')
                full_name = await user_info.inner_text() if user_info else "Unknown"
                
                # æœ¬æ–‡
                content_elem = await tweet.query_selector('[data-testid="tweetText"]')
                content = await content_elem.inner_text() if content_elem else ""

                # çµ±è¨ˆ
                group_label = await tweet.query_selector('div[role="group"]')
                stats_text = await group_label.get_attribute('aria-label') if group_label else ""

                metrics = {
                    "replies": extract_number(stats_text, r"(\d+)ä»¶ã®ãƒªãƒ—ãƒ©ã‚¤"),
                    "retweets": extract_number(stats_text, r"(\d+)ä»¶ã®ãƒªãƒã‚¹ãƒˆ"),
                    "likes": extract_number(stats_text, r"(\d+)ä»¶ã®ã„ã„ã­"),
                    "views": extract_number(stats_text, r"([\d\.]+[ä¸‡å„„]?+)ä»¶ã®è¡¨ç¤º")
                }

                # ç”»åƒï¼ˆãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒã‚’é™¤å¤–ã—ã¦æŠ½å‡ºï¼‰
                img_elements = await tweet.query_selector_all('img[src*="media"]')
                images = []
                for img in img_elements:
                    src = await img.get_attribute('src')
                    if src and "profile_images" not in src:
                        images.append(src)

                # ãƒ„ã‚¤ãƒ¼ãƒˆURL
                link_elem = await tweet.query_selector('a[href*="/status/"]')
                tweet_url = f"https://x.com{await link_elem.get_attribute('href')}" if link_elem else ""

                if images and tweet_url:
                    results.append({
                        "member_id": member.get('id', 'unknown'),
                        "member_name": member['name'],
                        "author_name": full_name.split("\n")[0],
                        "author_id": full_name.split("\n")[1] if "\n" in full_name else "",
                        "content": content,
                        "metrics": metrics,
                        "images": list(set(images)),
                        "url": tweet_url,
                        "source": "X",
                        "collected_at": datetime.now().isoformat()
                    })
            except:
                continue
    except Exception as e:
        print(f"âŒ Error scraping {member['name']}: {e}")
    
    await page.close()
    return results

async def main():
    if not os.path.exists('members.json'):
        print("Error: members.json not found")
        return

    with open('members.json', 'r', encoding='utf-8') as f:
        members = json.load(f)

    data_file = 'collect.json'
    all_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try:
                all_data = json.load(f)
            except:
                all_data = []
    
    existing_urls = {item['url'] for item in all_data}

    async with async_playwright() as p:
        # æµ·å¤–ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ€ªã—ã¾ã‚Œãªã„ã‚ˆã†ã€è¨€èªè¨­å®šç­‰ã‚’æŒ‡å®š
        browser = await p.chromium.launch(headless=True)
        
        if not os.path.exists('auth.json'):
            print("Error: auth.json not found.")
            await browser.close()
            return

        context = await browser.new_context(
            storage_state="auth.json",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            locale="ja-JP"
        )
        
        # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã§ã®ãƒ­ãƒƒã‚¯ã‚’é¿ã‘ã‚‹ãŸã‚ã€1äººãšã¤ã‚†ã£ãã‚Šå‡¦ç†
        for member in members:
            new_tweets = await scrape_vspo_cosplay(context, member)
            added_count = 0
            for t in new_tweets:
                if t['url'] not in existing_urls:
                    all_data.append(t)
                    existing_urls.add(t['url'])
                    added_count += 1
            print(f"âœ¨ Added {added_count} new items for {member['name']}")
            await asyncio.sleep(random.uniform(2, 5))
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        await browser.close()
        print(f"ğŸš€ All process finished! Total database: {len(all_data)}")

if __name__ == "__main__":
    asyncio.run(main())
