import json
import os
import re
import asyncio
import random
from datetime import datetime
from playwright.async_api import async_playwright

def extract_number(text, pattern):
    if not text: return "0"
    match = re.search(pattern, text)
    return match.group(1) if match else "0"

async def scrape_vspo_cosplay(context, member):
    results = []
    page = await context.new_page()
    await page.set_viewport_size({"width": 1280, "height": 800})

    query = f"{member['name']} ã‚³ã‚¹ãƒ—ãƒ¬"
    url = f"https://x.com/search?q={query}&src=typed_query&f=live"
    
    print(f"--- Searching for: {member['name']} ---")
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼šdomcontentloadedã§æ—©ã‚ã«æ¬¡ã¸
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        await asyncio.sleep(3) # èª­ã¿è¾¼ã¿å¾…ã¡

        if "login" in page.url:
            print(f"âš ï¸ Login page detected. Skipping.")
            return []

        try:
            await page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)
        except:
            print(f"âŒ No tweets found (Timeout).")
            return []

        # ç”»åƒã‚’èª­ã¿è¾¼ã¾ã›ã‚‹ãŸã‚ã«å°‘ã—ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        for _ in range(3):
            await page.mouse.wheel(0, 1000)
            await asyncio.sleep(1)

        tweets = await page.query_selector_all('article[data-testid="tweet"]')
        print(f"âœ… Found {len(tweets)} tweets in DOM")

        for i, tweet in enumerate(tweets):
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å
                user_elem = await tweet.query_selector('[data-testid="User-Name"]')
                full_name = await user_elem.inner_text() if user_elem else "Unknown"
                
                # åºƒå‘Šã‚¹ã‚­ãƒƒãƒ—
                if "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³" in full_name or "Ad" in full_name:
                    continue

                # æœ¬æ–‡
                content_elem = await tweet.query_selector('[data-testid="tweetText"]')
                content = await content_elem.inner_text() if content_elem else ""

                # ç”»åƒæŠ½å‡ºã®å¼·åŒ–ï¼šdata-testid="tweetPhoto" ã®ä¸­ã® img ã‚’å„ªå…ˆçš„ã«æŽ¢ã™
                images = []
                photo_divs = await tweet.query_selector_all('div[data-testid="tweetPhoto"] img')
                
                for img in photo_divs:
                    src = await img.get_attribute('src')
                    if src: images.append(src)
                
                # ã‚‚ã—ä¸Šè¨˜ã§è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€æ±Žç”¨çš„ãª img ã‚¿ã‚°ã‚‚æŽ¢ã™ï¼ˆãƒ—ãƒ­ãƒ•ç”»åƒç­‰ã¯é™¤å¤–ï¼‰
                if not images:
                    all_imgs = await tweet.query_selector_all('img')
                    for img in all_imgs:
                        src = await img.get_attribute('src')
                        # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚µãƒ¼ãƒãƒ¼(pbs.twimg.com)ã®ç”»åƒã§ã€ã‹ã¤ãƒ—ãƒ­ãƒ•ç”»åƒã§ãªã„ã‚‚ã®
                        if src and "pbs.twimg.com/media" in src and "profile_images" not in src:
                            images.append(src)

                # é‡è¤‡æŽ’é™¤
                images = list(set(images))

                # URLå–å¾—
                link_elem = await tweet.query_selector('a[href*="/status/"]')
                tweet_url = f"https://x.com{await link_elem.get_attribute('href')}" if link_elem else ""

                # ä¿å­˜åˆ¤å®š
                if images and tweet_url:
                    results.append({
                        "member_id": member.get('id', 'unknown'),
                        "member_name": member['name'],
                        "content": content,
                        "images": images,
                        "url": tweet_url,
                        "collected_at": datetime.now().isoformat()
                    })
                    print(f"  â­• Saved tweet from {full_name.splitlines()[0]}: {len(images)} images")
                else:
                    # ãªãœä¿å­˜ã•ã‚Œãªã‹ã£ãŸã‹ãƒ­ã‚°ã«å‡ºã™
                    reason = []
                    if not images: reason.append("No images")
                    if not tweet_url: reason.append("No URL")
                    print(f"  Start analyzing tweet {i+1}... Skip: {', '.join(reason)}")

            except Exception as e:
                print(f"  âŒ Error processing tweet {i+1}: {e}")
                continue

    except Exception as e:
        print(f"âŒ Error scraping {member['name']}: {e}")
    
    await page.close()
    return results

async def main():
    if not os.path.exists('members.json'): return
    with open('members.json', 'r', encoding='utf-8') as f:
        members = json.load(f)

    # ãƒ†ã‚¹ãƒˆç”¨ï¼šå…¨å“¡ã‚„ã‚‹ã¨é•·ã„ã®ã§ã€æœ€åˆã®3äººã ã‘è©¦ã™ãªã‚‰ä»¥ä¸‹ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤
    # members = members[:3]

    data_file = 'collect.json'
    all_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: all_data = json.load(f)
            except: all_data = []
    
    existing_urls = {item['url'] for item in all_data}

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        if not os.path.exists('auth.json'):
            print("Error: auth.json not found.")
            await browser.close()
            return

        context = await browser.new_context(
            storage_state="auth.json",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        
        for member in members:
            new_tweets = await scrape_vspo_cosplay(context, member)
            count = 0
            for t in new_tweets:
                if t['url'] not in existing_urls:
                    all_data.append(t)
                    existing_urls.add(t['url'])
                    count += 1
            if count > 0:
                print(f"âœ¨ Added {count} new items for {member['name']}")
            
            await asyncio.sleep(random.uniform(2, 4))
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        await browser.close()
        print(f"ðŸš€ Finished! Total items in DB: {len(all_data)}")

if __name__ == "__main__":
    asyncio.run(main())
