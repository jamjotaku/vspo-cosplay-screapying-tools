import json
import os
import asyncio
import random
import requests
from io import BytesIO
from datetime import datetime
from playwright.async_api import async_playwright
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# â– â– â–  è¨­å®šï¼šCLIPãƒ¢ãƒ‡ãƒ«ï¼ˆCPUã§ã‚‚å‹•ãè»½é‡ç‰ˆï¼‰ â– â– â– 
MODEL_ID = "openai/clip-vit-base-patch32"
print("ğŸš€ Loading Local AI (CLIP)... This takes a moment.")
try:
    model = CLIPModel.from_pretrained(MODEL_ID)
    processor = CLIPProcessor.from_pretrained(MODEL_ID)
    print("âœ… CLIP Model Loaded!")
except Exception as e:
    print(f"âš ï¸ Failed to load CLIP: {e}")
    model = None

def check_image_locally(image_url, member_name):
    """
    ç”»åƒURLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€CLIPã§ã€Œãã®ã‚­ãƒ£ãƒ©ã®ã‚³ã‚¹ãƒ—ãƒ¬ã‹ï¼Ÿã€ã‚’åˆ¤å®šã™ã‚‹
    """
    if model is None: return True # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯ã‚¹ãƒ«ãƒ¼ã—ã¦ä¿å­˜
    
    try:
        # ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šä»˜ãï¼‰
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(image_url, headers=headers, timeout=10)
        if response.status_code != 200: return False
        
        image = Image.open(BytesIO(response.content)).convert("RGB")
        
        # åˆ¤å®šãƒ©ãƒ™ãƒ«ï¼ˆè‹±èªã®ã»ã†ãŒç²¾åº¦ãŒè‰¯ã„ï¼‰
        # 0ç•ªç›®ãŒã€Œæ­£è§£ã€ã®åŸºæº–
        labels = [
            f"a cosplay photo of {member_name}",
            "a screenshot of a video game or anime",
            "text or merchandise or random object"
        ]

        inputs = processor(text=labels, images=image, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        
        probs = outputs.logits_per_image.softmax(dim=1)
        top_index = probs.argmax().item()
        
        # 0ç•ªç›®ã®ç¢ºç‡ãŒä¸€ç•ªé«˜ã‘ã‚Œã°åˆæ ¼
        if top_index == 0:
            return True
        else:
            return False

    except Exception:
        return True # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã®ãŸã‚æ®‹ã™

async def scrape_vspo_cosplay(context, member):
    results = []
    page = await context.new_page()
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆç”»åƒãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼‰
    query = f"{member['name']} ã‚³ã‚¹ãƒ—ãƒ¬"
    url = f"https://x.com/search?q={query}&src=typed_query&f=live"
    
    print(f"--- [X] Searching: {member['name']} ---")
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        await asyncio.sleep(5) # èª­ã¿è¾¼ã¿å¾…ã¡

        # ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—
        tweets = await page.query_selector_all('article[data-testid="tweet"]')
        print(f"   Found {len(tweets)} tweets")

        for tweet in tweets[:15]: # 1äººã‚ãŸã‚Šæœ€å¤§15ä»¶ãƒã‚§ãƒƒã‚¯
            try:
                # æœ¬æ–‡å–å¾—
                content_elem = await tweet.query_selector('[data-testid="tweetText"]')
                content = await content_elem.inner_text() if content_elem else ""
                
                # ãƒã‚¤ã‚ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
                if any(x in content for x in ["è­²æ¸¡", "è²·å–", "äº¤æ›", "ã‚°ãƒƒã‚º"]): continue

                # ç”»åƒURLå–å¾—
                images = []
                photo_divs = await tweet.query_selector_all('div[data-testid="tweetPhoto"] img')
                for img in photo_divs:
                    src = await img.get_attribute('src')
                    if src: images.append(src)
                
                # ãƒªãƒ³ã‚¯å–å¾—
                link_elem = await tweet.query_selector('a[href*="/status/"]')
                tweet_url = f"https://x.com{await link_elem.get_attribute('href')}" if link_elem else ""

                if images and tweet_url:
                    # â˜…AIåˆ¤å®šï¼ˆ1æšç›®ã ã‘ãƒã‚§ãƒƒã‚¯ï¼‰
                    if check_image_locally(images[0], member['name']):
                        results.append({
                            "member_name": member['name'],
                            "content": content,
                            "images": images,
                            "url": tweet_url,
                            "source": "X",
                            "collected_at": datetime.now().isoformat()
                        })
                        print(f"   âœ… Saved: {member['name']}")
                    else:
                        print(f"   ğŸ—‘ï¸ Rejected by AI")
            except Exception:
                continue

    except Exception as e:
        print(f"âŒ Error: {e}")
    
    await page.close()
    return results

async def main():
    # ãƒ¡ãƒ³ãƒãƒ¼ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
    if not os.path.exists('members.json'): return
    with open('members.json', 'r', encoding='utf-8') as f:
        members = json.load(f)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_file = 'collect.json'
    all_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: all_data = json.load(f)
            except: all_data = []
    
    existing_urls = {item['url'] for item in all_data}

    # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        # auth.json ãŒãªã„å ´åˆã¯çµ‚äº†
        if not os.path.exists('auth.json'):
            print("Error: auth.json not found.")
            await browser.close()
            return

        context = await browser.new_context(
            storage_state="auth.json",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        
        for member in members:
            new_tweets = await scrape_vspo_cosplay(context, member)
            count = 0
            for t in new_tweets:
                if t['url'] not in existing_urls:
                    all_data.append(t)
                    existing_urls.add(t['url'])
                    count += 1
            
            await asyncio.sleep(random.uniform(3, 6)) # BANå¯¾ç­–ã®ä¼‘æ†©
        
        # ä¿å­˜
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        await browser.close()
        print("ğŸ‰ X Scraping Finished!")

if __name__ == "__main__":
    asyncio.run(main())