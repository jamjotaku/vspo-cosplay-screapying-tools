import json
import os
import re
import asyncio
import random
from datetime import datetime
from playwright.async_api import async_playwright

def extract_number(text, pattern):
    if not text: return "0"
    match = re.search(pattern, text)
    return match.group(1) if match else "0"

async def scrape_vspo_cosplay(context, member):
    results = []
    page = await context.new_page()
    await page.set_viewport_size({"width": 1280, "height": 800})

    query = f"{member['name']} ã‚³ã‚¹ãƒ—ãƒ¬"
    url = f"https://x.com/search?q={query}&src=typed_query&f=live"
    
    print(f"--- Searching for: {member['name']} ---")
    try:
        # ã€ä¿®æ­£ç‚¹ã€‘networkidle (é€šä¿¡å®Œäº†å¾…ã¡) ã‚’ã‚„ã‚ã€domcontentloaded (è¡¨ç¤ºå¾…ã¡) ã«å¤‰æ›´
        # ã“ã‚Œã«ã‚ˆã‚Šã€ç„¡é™ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é˜²ãã¾ã™
        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
        
        # èª­ã¿è¾¼ã¿ã®ä½™éŸ»ã¨ã—ã¦å°‘ã—ã ã‘å¾…ã¤
        await asyncio.sleep(3)

        # ãƒ­ã‚°ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
        if "login" in page.url:
            print(f"âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ãŒè§£é™¤ã•ã‚Œã¦ã„ã¾ã™ã€‚")
            return []

        # ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§10ç§’ï¼‰
        try:
            await page.wait_for_selector('article[data-testid="tweet"]', timeout=10000)
        except:
            print(f"âŒ ãƒ„ã‚¤ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆæ¤œç´¢çµæœ0ä»¶ã€ã¾ãŸã¯èª­è¾¼ã‚¨ãƒ©ãƒ¼ï¼‰")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚¹ã‚¯ã‚·ãƒ§ã‚’ä¿å­˜
            await page.screenshot(path=f"error_{member['id']}.png")
            return []

        # å°‘ã—ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¾ã›ã‚‹
        await page.mouse.wheel(0, 1000)
        await asyncio.sleep(1)

        tweets = await page.query_selector_all('article[data-testid="tweet"]')
        print(f"âœ… Found {len(tweets)} tweets")

        for tweet in tweets:
            try:
                user_info = await tweet.query_selector('[data-testid="User-Name"]')
                full_name = await user_info.inner_text() if user_info else "Unknown"
                
                content_elem = await tweet.query_selector('[data-testid="tweetText"]')
                content = await content_elem.inner_text() if content_elem else ""

                # åºƒå‘Šãƒ„ã‚¤ãƒ¼ãƒˆã‚’é™¤å¤–
                if "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³" in full_name or "Ad" in full_name:
                    continue

                group_label = await tweet.query_selector('div[role="group"]')
                stats_text = await group_label.get_attribute('aria-label') if group_label else ""

                metrics = {
                    "replies": extract_number(stats_text, r"(\d+)ä»¶ã®ãƒªãƒ—ãƒ©ã‚¤"),
                    "retweets": extract_number(stats_text, r"(\d+)ä»¶ã®ãƒªãƒã‚¹ãƒˆ"),
                    "likes": extract_number(stats_text, r"(\d+)ä»¶ã®ã„ã„ã­"),
                    "views": extract_number(stats_text, r"([\d\.]+[ä¸‡å„„]?+)ä»¶ã®è¡¨ç¤º")
                }

                img_elements = await tweet.query_selector_all('img[src*="media"]')
                images = []
                for img in img_elements:
                    src = await img.get_attribute('src')
                    if src and "profile_images" not in src:
                        images.append(src)

                link_elem = await tweet.query_selector('a[href*="/status/"]')
                tweet_url = f"https://x.com{await link_elem.get_attribute('href')}" if link_elem else ""

                if images and tweet_url:
                    results.append({
                        "member_id": member.get('id', 'unknown'),
                        "member_name": member['name'],
                        "content": content,
                        "metrics": metrics,
                        "images": list(set(images)),
                        "url": tweet_url,
                        "source": "X",
                        "collected_at": datetime.now().isoformat()
                    })
            except:
                continue
    except Exception as e:
        print(f"âŒ Error scraping {member['name']}: {e}")
    
    await page.close()
    return results

async def main():
    if not os.path.exists('members.json'): return

    with open('members.json', 'r', encoding='utf-8') as f:
        members = json.load(f)

    # å‹•ä½œç¢ºèªã®ãŸã‚ã€æœ€åˆã®3äººã ã‘ãƒ†ã‚¹ãƒˆã—ãŸã„å ´åˆã¯ã“ã“ã‚’æœ‰åŠ¹ã«
    # members = members[:3] 

    data_file = 'collect.json'
    all_data = []
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            try: all_data = json.load(f)
            except: all_data = []
    
    existing_urls = {item['url'] for item in all_data}

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        if not os.path.exists('auth.json'):
            print("Error: auth.json not found.")
            await browser.close()
            return

        context = await browser.new_context(
            storage_state="auth.json",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        
        for member in members:
            new_tweets = await scrape_vspo_cosplay(context, member)
            count = 0
            for t in new_tweets:
                if t['url'] not in existing_urls:
                    all_data.append(t)
                    existing_urls.add(t['url'])
                    count += 1
            if count > 0:
                print(f"âœ¨ Added {count} new items for {member['name']}")
            
            # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹å¯¾ç­–ã®ä¼‘æ†©ï¼ˆ2ã€œ4ç§’ï¼‰
            await asyncio.sleep(random.uniform(2, 4))
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        await browser.close()
        print(f"ğŸš€ Finished! Total items: {len(all_data)}")

if __name__ == "__main__":
    asyncio.run(main())
